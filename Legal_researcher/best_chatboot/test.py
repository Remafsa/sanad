# !pip install openai
# !pip install -U langchain-community
# from langchain_community.document_loaders import TextLoader
# !pip install langchain
# !pip install python-dotenv


import os
import openai
import sys
sys.path.append('../..')

from dotenv import load_dotenv, find_dotenv

load_dotenv()

# Get the OpenAI API key from the environment variables
openai_api_key = os.getenv('OPENAI_API_KEY')

import json
from pathlib import Path
from pprint import pprint

# !pip install jq

import json
from pathlib import Path
from langchain_core.documents import Document


directory = Path("/content/drive/MyDrive/best_chatboot")

documents = []
for file_path in directory.glob('*.json'):
    with open(file_path, "r", encoding="utf-8") as json_file:
        data = json.load(json_file)
        metadata = {
            'Ø§Ù„Ø±Ø§Ø¨Ø·': data['Ø§Ù„Ø±Ø§Ø¨Ø·'],
            'Ø§Ù„Ù…Ù„Ù': str(file_path),
            'Ø§Ù„Ù†Ø¸Ø§Ù…': data['Ø§Ù„Ù†Ø¸Ø§Ù…'],
        }
        for chapter, articles in data['Ø§Ù„ÙØµÙˆÙ„'].items():
            for article, content in articles.items():
                documents.append(Document(content, metadata={**metadata, 'Ø§Ù„ÙØµÙ„': chapter, 'Ø§Ù„Ù…Ø§Ø¯Ø©': article}))

from langchain.text_splitter import TokenTextSplitter

# !pip install tiktoken

from langchain.embeddings.openai import OpenAIEmbeddings
embedding = OpenAIEmbeddings()

import os

directory = "/content/drive/MyDrive/best_chatboot"
filenames = []

for filename in os.listdir(directory):
    if filename.endswith(".json"):
        filenames.append(os.path.join(directory, filename))

embedding1 = embedding.embed_documents(filenames)

from langchain.vectorstores import Chroma

from langchain.text_splitter import CharacterTextSplitter
text_splitter = CharacterTextSplitter(
    separator="\n",
    chunk_size=1737,
    chunk_overlap=150,
    length_function=len
)

splits = text_splitter.split_documents(documents)

# !pip install chromadb

retriever = Chroma(persist_directory="/content/drive/MyDrive/docs2_best/chroma", embedding_function=embedding).as_retriever()





import datetime
current_date = datetime.datetime.now().date()
if current_date < datetime.date(2023, 9, 2):
    llm_name = "gpt-3.5-turbo-0301"
else:
    llm_name = "gpt-3.5-turbo"
print(llm_name)

from langchain.chat_models import ChatOpenAI
llm = ChatOpenAI(model_name=llm_name, temperature=0)


from langchain.chains import RetrievalQA

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff"  # or any other chain type you want to use
)


################here ii

# from langchain.prompts import PromptTemplate

# # Build prompt
# template = """Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ Ø§Ù„ØªØ§Ù„ÙŠØ© Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©. Ø¥Ø°Ø§ ÙƒÙ†Øª Ù„Ø§ ØªØ¹Ø±Ù Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©ØŒ Ù‚Ù„ ÙÙ‚Ø· Ø£Ù†Ùƒ Ù„Ø§ ØªØ¹Ø±ÙØŒ ÙˆÙ„Ø§ ØªØ­Ø§ÙˆÙ„ Ø§Ø®ØªÙ„Ø§Ù‚ Ø¥Ø¬Ø§Ø¨Ø©.  Ù‚Ù„ Ø¯Ø§Ø¦Ù…Ù‹Ø§ "Ø´ÙƒØ±Ù‹Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„!" ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø¬ÙˆØ§Ø¨.
# {context}
# Question: {question}
# Helpful Answer:"""
# QA_CHAIN_PROMPT = PromptTemplate.from_template(template)


# qa_chain = RetrievalQA.from_chain_type(
#     llm=llm,
#     retriever=retriever,  # Use the retriever you initialized
#     return_source_documents=True,
#     chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
# )


#######USER INTERFACE###

# import streamlit as st
# st.title(" âš–ï¸Ø§Ø³Ø£Ù„Ù†ÙŠ Ø¹Ù† Ø§Ù„Ø§Ù†Ø¸Ù…Ø© ÙÙŠ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©")

# question = st.text_input("ğŸ’¬Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§")
# if question:
#     result = qa_chain({"query": question})
#     answer = result["result"]
#     st.write(f"Answer: {answer}")

    # '''st.write("Source Documents:")
    # for doc in result['source_documents']:
    #     st.write(f"- {doc.metadata['Ø§Ù„Ù…Ù„Ù']} (Chapter: {doc.metadata['Ø§Ù„ÙØµÙ„']}, Article: {doc.metadata['Ø§Ù„Ù…Ø§Ø¯Ø©']})")'''


# from langchain_community.chat_message_histories import (
#     StreamlitChatMessageHistory,
# )

# history = StreamlitChatMessageHistory(key="chat_messages")

# history.add_user_message("hi!")
# history.add_ai_message("whats up?")

# history.messages

# # Optionally, specify your own session_state key for storing messages
# msgs = StreamlitChatMessageHistory(key="special_app_key")

# if len(msgs.messages) == 0:
#     msgs.add_ai_message("How can I help you?")

# from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
# from langchain_core.runnables.history import RunnableWithMessageHistory
# from langchain_openai import ChatOpenAI

# prompt = ChatPromptTemplate.from_messages(
#     [
#         ("system", "You are an AI chatbot having a conversation with a human."),
#         MessagesPlaceholder(variable_name="history"),
#         ("human", "{question}"),
#     ]
# )

# chain = prompt | ChatOpenAI()


# chain_with_history = RunnableWithMessageHistory(
#     chain,
#     lambda session_id: msgs,  # Always return the instance created earlier
#     input_messages_key="question",
#     history_messages_key="history",
# )

# import streamlit as st

# for msg in msgs.messages:
#     st.chat_message(msg.type).write(msg.content)

# if prompt := st.chat_input():
#     st.chat_message("human").write(prompt)

#     # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.
#     config = {"configurable": {"session_id": "any"}}
#     response = chain_with_history.invoke({"question": prompt}, config)
#     st.chat_message("ai").write(response.content)

##########################best UI is unser############
# import streamlit as st
# from langchain.prompts import PromptTemplate
# from langchain_community.chat_message_histories import StreamlitChatMessageHistory
# from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
# from langchain_core.runnables.history import RunnableWithMessageHistory
# from langchain_openai import ChatOpenAI
# from langchain.chains import RetrievalQA
# from langchain.llms import OpenAI
# #from langchain.retrievers import DummyRetriever  # Use a real retriever in production

# # Initialize chat message history
# msgs = StreamlitChatMessageHistory(key="special_app_key")

# if len(msgs.messages) == 0:
#     msgs.add_ai_message("ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒØŸ")

# # Define chat prompt
# chat_prompt = ChatPromptTemplate.from_messages(
#     [
#         ("system", "You are an AI chatbot having a conversation with a human."),
#         MessagesPlaceholder(variable_name="history"),
#         ("human", "{question}"),
#     ]
# )

# # Initialize the OpenAI chat model for conversation
# chat_llm = ChatOpenAI()

# # Combine the chat prompt with the model
# chat_chain = chat_prompt | chat_llm

# # Wrap the chain with message history handling
# chain_with_history = RunnableWithMessageHistory(
#     chat_chain,
#     lambda session_id: msgs,
#     input_messages_key="question",
#     history_messages_key="history",
# )

# # Define the QA prompt template
# template = """Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ Ø§Ù„ØªØ§Ù„ÙŠØ© Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©. Ø¥Ø°Ø§ ÙƒÙ†Øª Ù„Ø§ ØªØ¹Ø±Ù Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©ØŒ Ù‚Ù„ ÙÙ‚Ø· Ø£Ù†Ùƒ Ù„Ø§ ØªØ¹Ø±ÙØŒ ÙˆÙ„Ø§ ØªØ­Ø§ÙˆÙ„ Ø§Ø®ØªÙ„Ø§Ù‚ Ø¥Ø¬Ø§Ø¨Ø©. Ø§Ø³ØªØ®Ø¯Ù… Ø«Ù„Ø§Ø« Ø¬Ù…Ù„ ÙƒØ­Ø¯ Ø£Ù‚ØµÙ‰. Ø§Ø¬Ø¹Ù„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø®ØªØµØ±Ø© Ù‚Ø¯Ø± Ø§Ù„Ø¥Ù…ÙƒØ§Ù†. ÙˆØ§Ø¬Ø¨ Ø¨Ø§Ù„ØªØ±Ø­ÙŠØ¨ ÙÙŠ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙˆÙÙŠ Ù†Ù‡Ø§ÙŠØ© ÙƒÙ„ Ø¬ÙˆØ§Ø¨ Ù‚Ù„ Ø§Ù†Ø§ Ù‡Ù†Ø§ Ø¯Ø§Ø¦Ù…Ø§ Ù„Ø®Ø¯Ù…ØªÙƒ." ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø¬ÙˆØ§Ø¨.
# {context}
# Question: {question}
# Helpful Answer:"""
# QA_CHAIN_PROMPT = PromptTemplate.from_template(template)

# # Set up your LLM and retriever (dummy retriever for example purposes)
# llm = OpenAI()  # Replace with your actual LLM
# #retriever = DummyRetriever()  # Replace with your actual retriever

# retriever = Chroma(persist_directory="/content/drive/MyDrive/docs2_best/chroma", embedding_function=embedding).as_retriever()
# # Initialize the retrieval-based QA chain
# qa_chain = RetrievalQA.from_chain_type(
#     llm=llm,
#     retriever=retriever,
#     return_source_documents=True,
#     chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
# )

# # Streamlit interface
# st.title('âš–ï¸Ø§Ø³Ø£Ù„Ù†ÙŠ Ø¹Ù† Ø§Ù„Ø§Ù†Ø¸Ù…Ø© ÙÙŠ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©')

# # Display chat history
# for msg in msgs.messages:
#     st.chat_message(msg.type).write(msg.content)

# # Input field for user to type a message
# if user_input := st.chat_input():
#     st.chat_message("user").write(user_input)
#     msgs.add_user_message(user_input)

#     # Generate response using the retrieval-based QA chain
#     result = qa_chain({"query": user_input})
#     answer = result["result"]
#     st.chat_message("ai").write(answer)
#     msgs.add_ai_message(answer)


import streamlit as st
from langchain.prompts import PromptTemplate
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI
#from langchain.retrievers import DummyRetriever  # Use a real retriever in production

# Initialize chat message history
msgs = StreamlitChatMessageHistory(key="special_app_key")

if len(msgs.messages) == 0:
    msgs.add_ai_message("ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒØŸ")

# Define chat prompt
chat_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are an AI chatbot having a conversation with a human."),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{question}"),
    ]
)

# Initialize the OpenAI chat model for conversation
chat_llm = ChatOpenAI()

# Combine the chat prompt with the model
chat_chain = chat_prompt | chat_llm

# Wrap the chain with message history handling
chain_with_history = RunnableWithMessageHistory(
    chat_chain,
    lambda session_id: msgs,
    input_messages_key="question",
    history_messages_key="history",
)

# Define the QA prompt template
template = """Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ Ø§Ù„ØªØ§Ù„ÙŠØ© Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©. Ø¥Ø°Ø§ ÙƒÙ†Øª Ù„Ø§ ØªØ¹Ø±Ù Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©ØŒ Ù‚Ù„ ÙÙ‚Ø· Ø£Ù†Ùƒ Ù„Ø§ ØªØ¹Ø±ÙØŒ ÙˆÙ„Ø§ ØªØ­Ø§ÙˆÙ„ Ø§Ø®ØªÙ„Ø§Ù‚ Ø¥Ø¬Ø§Ø¨Ø©.ÙˆØ§Ø°Ø§ Ø³Ø§Ù„ Ø¹Ù† Ø­Ø§Ù„Ùƒ Ù‚Ù„ Ø§Ù†Ø§ Ø¨Ø®ÙŠØ± ÙˆØ§Ø¬Ø¨ Ø¨Ø§Ù„ØªØ±Ø­ÙŠØ¨ ÙÙŠ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙˆÙÙŠ Ù†Ù‡Ø§ÙŠØ© ÙƒÙ„ Ø¬ÙˆØ§Ø¨ Ù‚Ù„ Ø§Ù†Ø§ Ù‡Ù†Ø§ Ø¯Ø§Ø¦Ù…Ø§ Ù„Ø®Ø¯Ù…ØªÙƒ..
{context}
Question: {question}
Helpful Answer:"""
QA_CHAIN_PROMPT = PromptTemplate.from_template(template)

# Set up your LLM and retriever (dummy retriever for example purposes)
llm = OpenAI()  # Replace with your actual LLM
#retriever = DummyRetriever()  # Replace with your actual retriever

retriever = Chroma(persist_directory="/content/drive/MyDrive/docs2_best/chroma", embedding_function=embedding).as_retriever()
# Initialize the retrieval-based QA chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True,
    chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
)

# Add custom CSS for chat messages
st.markdown("""
<style>

.title {
    text-align: center;
}

.chat-message-user {
    display: flex;
    align-items: center;
    background-color: #82C42E;
    border-radius: 10px;
    padding: 10px;
    margin-bottom: 10px;
}
.chat-message-user::before {
    content: "ğŸ‘¤"; /* Custom icon for user */
    margin-right: 10px;

}

.chat-message-ai {
    display: flex;
    align-items: center;
    background-color: #9EA45C;
    border-radius: 10px;
    padding: 10px;
    margin-bottom: 10px;
}
.chat-message-ai::before {
    content: "ğŸ¤–"; /* Custom icon for AI */
    margin-right: 10px;
}
</style>
""", unsafe_allow_html=True)

st.image("https://e3.365dm.com/23/09/768x432/skynews-judge-stock_6285365.jpg?20230915095121", use_column_width=True)

# Streamlit interface
#st.title('âš–ï¸Ø§Ø³Ø£Ù„Ù†ÙŠ Ø¹Ù† Ø§Ù„Ø§Ù†Ø¸Ù…Ø© ÙÙŠ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©')
st.markdown('<h1 class="title">âš–ï¸ Ø§Ø³Ø£Ù„Ù†ÙŠ Ø¹Ù† Ø§Ù„Ø£Ù†Ø¸Ù…Ø© ÙÙŠ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©</h1>', unsafe_allow_html=True)


# Display chat history
for msg in msgs.messages:
    if msg.type == "user":
        st.markdown(f'<div class="chat-message-user">{msg.content}</div>', unsafe_allow_html=True)
    else:
        st.markdown(f'<div class="chat-message-ai">{msg.content}</div>', unsafe_allow_html=True)

# Input field for user to type a message
if user_input := st.chat_input():
    st.markdown(f'<div class="chat-message-user">{user_input}</div>', unsafe_allow_html=True)
    msgs.add_user_message(user_input)

    docs_ = retriever.get_relevant_documents(user_input)
    # Generate response using the retrieval-based QA chain
    result = qa_chain({"query": user_input})
    answer = result["result"]
    st.markdown(f'<div class="chat-message-ai">{answer}</div>', unsafe_allow_html=True)
    msgs.add_ai_message(answer)
